{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import statsmodels.api as sm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, roc_curve, precision_recall_curve, confusion_matrix #, plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, log_loss, f1_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join('..', 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import s05_2_feature_engineering\n",
    "importlib.reload(s05_2_feature_engineering)\n",
    "from s05_2_feature_engineering import build_polynomials, transform_label, treat_skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = os.path.join('..', 'data', '03_processed')\n",
    "models_reports = os.path.join('..', 'data', '04_models')\n",
    "model_outputs = os.path.join('..', 'data', '05_model_output')\n",
    "reports = os.path.join('..', 'data', '06_reporting')\n",
    "\n",
    "X_train           = pd.read_csv(os.path.join(inputs, 'X_train.csv'), index_col='id')\n",
    "X_test            = pd.read_csv(os.path.join(inputs, 'X_test.csv'), index_col='id')\n",
    "X_train_onehot    = pd.read_csv(os.path.join(inputs, 'X_train_onehot.csv'), index_col='id')\n",
    "X_test_onehot     = pd.read_csv(os.path.join(inputs, 'X_test_onehot.csv'), index_col='id')\n",
    "y_train           = pd.read_csv(os.path.join(inputs, 'y_train.csv'), index_col='id')\n",
    "y_test            = pd.read_csv(os.path.join(inputs, 'y_test.csv'), index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "# from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating final score with a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformations(X_set, y_set, cols,\n",
    "               build_polynomals_method=False, \n",
    "                label_transformation_type=None, do_treat_skewness=False,\n",
    "               imputation=None, scaler=None,\n",
    "               ):\n",
    "    X_set = X_set.copy()\n",
    "    print(cols)\n",
    "    print(X_set.columns.to_list())\n",
    "    X_set = X_set[cols]\n",
    "    \n",
    "#     if build_polynomals_method: \n",
    "#         X_train_set = build_polynomials(X_set, ProjectParameters().numerical_cols, method = build_polynomals_method)\n",
    "#     if label_transformation_type:\n",
    "#         y_set = transform_label(y_set, label_transformation_type)\n",
    "#     if do_treat_skewness:\n",
    "#         X_set = treat_skewness(X_set, set_name)\n",
    "        \n",
    "    if scaler:\n",
    "        X_set = scale.fit_transform(X_set)\n",
    "        X_set = pd.DataFrame(X_set, columns = X_set.columns)\n",
    "    if imputation:\n",
    "        X_set.fillna(imputation)\n",
    "    \n",
    "    return X_set, y_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X_train.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat', 'if_anomaly']\n",
      "['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat', 'if_anomaly']\n"
     ]
    }
   ],
   "source": [
    "label_transformation_type = None\n",
    "\n",
    "X_test, y_test = get_transformations(\n",
    "        X_test, y_test, columns,\n",
    "        build_polynomals_method=None, do_treat_skewness=False,\n",
    "        imputation=None, scaler=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load results\n",
    "Capture best parameters of chosen model which were obtained during cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_results(model_type):\n",
    "    filepath = os.path.join(model_outputs, model_type+'.json')\n",
    "    with open(filepath, 'r') as file:\n",
    "        json_results = json.load(file)\n",
    "    model = json_results[model_type]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model to apply on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(ml_model_type, X, y):\n",
    "    ml_model = load_json_results(ml_model_type)\n",
    "\n",
    "    if ml_model_type.startswith('tree_rf'):\n",
    "        params = {}\n",
    "        cols = ml_model['columns']\n",
    "        for k,v in ml_model['best_params'].items():\n",
    "            k = k[7:]\n",
    "            params[k] = v\n",
    "\n",
    "        model = RandomForestRegressor()\n",
    "\n",
    "    elif ml_model_type.startswith('tree_xgb'):\n",
    "        params = ml_model['best_params']\n",
    "        cols = ml_model['columns']\n",
    "        \n",
    "        model = XGBRegressor()\n",
    "        \n",
    "        X = X[cols]\n",
    "\n",
    "    for k,v in params.items(): \n",
    "        setattr(model, k, v)\n",
    "\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return model, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['tree_xgb', 'tree_xgb_num', 'tree_xgb_numcyc', 'tree_xgb_numcyc_smote', 'tree_rf']\n",
    "ml_model = 'tree_xgb'\n",
    "\n",
    "model, columns = build_model(ml_model, X_train, y_train)\n",
    "X_test = X_test[columns]\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate test score \n",
    "main metric: mean squared error for regression, f1_score for binary target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error: 268.846\n",
      "r2_score -3.052\n",
      "mean_absolute_error 14.318\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\nCLASSIFICATION_REPORT:\\n\", classification_report(y_test, y_pred))\n",
    "print('mean squared error:', round(mean_squared_error(y_test, y_pred), 3))\n",
    "print('r2_score', round(r2_score(y_test, y_pred),3))\n",
    "print('mean_absolute_error', round(mean_absolute_error(y_test, y_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample prediction\n",
    "predict first 10 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rebuild model for entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train.copy().append(X_test)\n",
    "y = y_train.copy().append(y_test)\n",
    "# y.extend(y_test)\n",
    "\n",
    "# ml_model = 'tree_randomforest'\n",
    "ml_model = 'tree_xgb'\n",
    "final_model = build_model(ml_model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save pickle of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(model_outputs, 'trained_model.pkl')\n",
    "with open(file, 'wb') as f:\n",
    "    pickle.dump(final_model, f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare estimated value with true value for sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = X.index.name\n",
    "ordered_users = pd.DataFrame(list(zip(X.index, y_pred, y_test)), columns = [index_name, 'estimated', 'true_value']).set_index(index_name)\n",
    "# ordered_users\n",
    "ordered_users.tail(10).sort_values(by='true_value', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort entries\n",
    "Retrieve IDs e and sort them by probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = X_train.index.name\n",
    "y_pred = y_pred_prob\n",
    "ordered_users = pd.DataFrame(list(zip(X_test.index, y_pred, y_test['y'])), columns = [index_name, 'probabilidade', 'valor_verdadeiro']).set_index(index_name)\n",
    "# ordered_users.sort_values(by='probabilidade', ascending=False, inplace=True)\n",
    "ordered_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the number of observations from testset and how many of them are positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_total = len(ordered_users)\n",
    "users_yes = len(ordered_users[ordered_users['valor_verdadeiro'] == 1])\n",
    "users_no = users_total - users_yes\n",
    "print('Size of test set:', users_total)\n",
    "print('number of positive cases:', users_yes, 'ou {}% do total'.format(round(users_yes/users_total*100, 2)))\n",
    "print('number of negative cases:', users_no, 'ou {}% do total'.format(round(users_no/users_total*100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we invested in only 8000 of users of higher probability to answer to campaign, we would achieve 3000 positive cases. But if we tried to increase that number to 4000 (1/3 de aumento!), we would need to double the investment, in other words, surpass 16000!\n",
    "\n",
    "Therefore, I would suggest a threshold close to 0.1 (10%). With that, we get many true positives while avoiding a high cost that could reduce ROI, given the consideration that the marginal cost would be low.\n",
    "\n",
    "Beyond that, the teams which are interested in evaluating the most relevant variables can consult feature importances results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
